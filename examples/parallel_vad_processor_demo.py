#!/usr/bin/env python3
"""
Cascadeé«˜å¹¶å‘å¹¶è¡ŒVADå¤„ç†å™¨æ¼”ç¤º
ä½¿ç”¨çœŸå®éŸ³é¢‘æ–‡ä»¶: "è¯·é—®ç”µåŠ¨æ±½è½¦å’Œä¼ ç»Ÿæ±½è½¦æ¯”èµ·æ¥å“ªä¸ªæ›´å¥½å•Šï¼Ÿ.wav"

å±•ç¤ºCascadeçš„çœŸæ­£å¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼š
1. å®Œæ•´çš„VADProcessoræµå¼å¤„ç†å™¨
2. å¤šçº¿ç¨‹å¹¶è¡ŒVADæ¨ç†ï¼ˆ1:1:1ç»‘å®šæ¶æ„ï¼‰
3. å¼‚æ­¥éŸ³é¢‘æµå¤„ç†
4. å®æ—¶æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡
5. èƒŒå‹æ§åˆ¶å’Œæµæ§æœºåˆ¶
6. é›¶æ‹·è´å†…å­˜ç®¡ç†
"""

import asyncio
import numpy as np
import time
import wave
from pathlib import Path
from typing import AsyncIterator, List, Dict, Any

# Cascadeæ ¸å¿ƒå¯¼å…¥
from cascade.types import (
    VADConfig, AudioConfig, AudioFormat, AudioChunk, VADResult
)
from cascade.processor import VADProcessor, VADProcessorConfig, create_vad_processor
from cascade.backends import create_vad_backend
from cascade._internal.thread_pool import VADThreadPoolConfig


class AudioStreamGenerator:
    """éŸ³é¢‘æµç”Ÿæˆå™¨ - å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºå¼‚æ­¥æµ"""
    
    def __init__(self, audio_data: np.ndarray, chunk_size: int, sample_rate: int):
        self.audio_data = audio_data
        self.chunk_size = chunk_size
        self.sample_rate = sample_rate
        
    async def generate_stream(self) -> AsyncIterator[np.ndarray]:
        """ç”ŸæˆéŸ³é¢‘æµ"""
        total_chunks = (len(self.audio_data) + self.chunk_size - 1) // self.chunk_size
        
        for i in range(total_chunks):
            start = i * self.chunk_size
            end = min(start + self.chunk_size, len(self.audio_data))
            
            chunk = self.audio_data[start:end]
            
            # ç¨å¾®å»¶è¿Ÿä»¥è§‚å¯Ÿå¼‚æ­¥å¤„ç†æ•ˆæœ
            await asyncio.sleep(0.001)  # 1mså»¶è¿Ÿ
            
            yield chunk
            
        print(f"ğŸµ éŸ³é¢‘æµç”Ÿæˆå®Œæˆ: {total_chunks}ä¸ªå—")


class ParallelPerformanceMonitor:
    """å¹¶è¡Œå¤„ç†æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.start_time = time.time()
        self.results: List[VADResult] = []
        self.processing_times: List[float] = []
        self.chunk_times: List[float] = []
        
    def add_result(self, result: VADResult):
        """æ·»åŠ å¤„ç†ç»“æœ"""
        self.results.append(result)
        self.chunk_times.append(time.time())
        
    def get_parallel_statistics(self) -> Dict[str, Any]:
        """è·å–å¹¶è¡Œå¤„ç†ç»Ÿè®¡"""
        if not self.results:
            return {}
            
        total_time = time.time() - self.start_time
        speech_results = [r for r in self.results if r.is_speech]
        
        # è®¡ç®—å¹¶è¡Œåº¦æŒ‡æ ‡
        if len(self.chunk_times) > 1:
            time_intervals = [self.chunk_times[i] - self.chunk_times[i-1] for i in range(1, len(self.chunk_times))]
            avg_interval = np.mean(time_intervals)
            max_interval = np.max(time_intervals)
            min_interval = np.min(time_intervals)
        else:
            avg_interval = max_interval = min_interval = 0.0
        
        return {
            "æ€»å¤„ç†æ—¶é—´": f"{total_time:.3f}ç§’",
            "å¤„ç†å—æ•°": len(self.results),
            "è¯­éŸ³å—æ•°": len(speech_results),
            "é™éŸ³å—æ•°": len(self.results) - len(speech_results),
            "è¯­éŸ³æ¯”ä¾‹": f"{len(speech_results)/len(self.results)*100:.1f}%",
            "å¹³å‡å—é—´éš”": f"{avg_interval*1000:.2f}ms",
            "æœ€å¤§å—é—´éš”": f"{max_interval*1000:.2f}ms", 
            "æœ€å°å—é—´éš”": f"{min_interval*1000:.2f}ms",
            "å¹¶è¡Œååé‡": f"{len(self.results)/total_time:.1f} chunks/s",
            "å®æ—¶å€ç‡": f"{(len(self.results) * 0.512) / total_time:.1f}x",  # å‡è®¾512mså—
            "å¹³å‡ç½®ä¿¡åº¦": f"{np.mean([r.probability for r in self.results]):.3f}"
        }


async def load_real_audio_file(audio_file: str) -> np.ndarray:
    """åŠ è½½çœŸå®éŸ³é¢‘æ–‡ä»¶ - åªå¤„ç†çœŸå®æ–‡ä»¶ï¼Œä¸ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
    audio_path = Path(audio_file)
    
    if not audio_path.exists():
        raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ '{audio_file}' ä¸å­˜åœ¨ï¼è¯·ç¡®ä¿æ–‡ä»¶åœ¨å½“å‰ç›®å½•ã€‚")
    
    print(f"ğŸ“‚ åŠ è½½çœŸå®éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    
    with wave.open(str(audio_path), 'rb') as wav_file:
        # è·å–éŸ³é¢‘å‚æ•°
        sample_rate = wav_file.getframerate()
        channels = wav_file.getnchannels()
        sample_width = wav_file.getsampwidth()
        n_frames = wav_file.getnframes()
        
        print(f"  ğŸ“Š éŸ³é¢‘å‚æ•°: {sample_rate}Hz, {channels}å£°é“, {sample_width*8}ä½, {n_frames/sample_rate:.2f}ç§’")
        
        # è¯»å–éŸ³é¢‘æ•°æ®
        raw_audio = wav_file.readframes(n_frames)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if sample_width == 2:  # 16ä½
            audio_data = np.frombuffer(raw_audio, dtype=np.int16).astype(np.float32) / 32768.0
        elif sample_width == 4:  # 32ä½
            audio_data = np.frombuffer(raw_audio, dtype=np.int32).astype(np.float32) / 2147483648.0
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä½æ·±: {sample_width*8}ä½")
        
        # å¦‚æœæ˜¯ç«‹ä½“å£°ï¼Œè½¬ä¸ºå•å£°é“
        if channels == 2:
            audio_data = audio_data.reshape(-1, 2).mean(axis=1)
        
        # é‡é‡‡æ ·åˆ°16kHzï¼ˆå¦‚æœéœ€è¦ï¼‰
        if sample_rate != 16000:
            print(f"  ğŸ”„ é‡é‡‡æ ·: {sample_rate}Hz -> 16000Hz")
            from scipy import signal
            audio_data = signal.resample(audio_data, int(len(audio_data) * 16000 / sample_rate))
        
        print(f"âœ… éŸ³é¢‘åŠ è½½æˆåŠŸ: {len(audio_data)/16000:.2f}ç§’, {len(audio_data)}ä¸ªé‡‡æ ·ç‚¹")
        return audio_data.astype(np.float32)


async def parallel_vad_demo():
    """é«˜å¹¶å‘å¹¶è¡ŒVADå¤„ç†æ¼”ç¤º"""
    
    print("ğŸš€ Cascadeé«˜å¹¶å‘å¹¶è¡ŒVADå¤„ç†å™¨æ¼”ç¤º")
    print("=" * 60)
    
    # === 1. é…ç½®é«˜æ€§èƒ½å¹¶è¡Œå¤„ç†å‚æ•° ===
    print("\nâš™ï¸ é…ç½®é«˜æ€§èƒ½å¹¶è¡Œå¤„ç†å‚æ•°...")
    
    # VADé…ç½® - ä¼˜åŒ–å¹¶è¡Œæ€§èƒ½
    vad_config = VADConfig(
        backend="silero",                 # ä½¿ç”¨Sileroåç«¯
        threshold=0.5,                    # ä¸­æ–‡è¯­éŸ³è¾ƒä½é˜ˆå€¼
        chunk_duration_ms=512,            # 512mså—ï¼ˆæœ€ä¼˜å¹¶è¡Œæ€§èƒ½ï¼‰
        overlap_ms=32,                    # 32msé‡å 
        min_speech_duration_ms=200,       # æœ€å°è¯­éŸ³æ®µ200ms
        workers=8                         # 8ä¸ªå·¥ä½œçº¿ç¨‹ï¼ˆé«˜å¹¶å‘ï¼‰
    )
    
    # éŸ³é¢‘é…ç½®
    audio_config = AudioConfig(
        sample_rate=16000,                # 16kHzæ ‡å‡†é‡‡æ ·ç‡
        channels=1,                       # å•å£°é“
        format=AudioFormat.WAV
    )
    
    # çº¿ç¨‹æ± é…ç½® - 1:1:1ç»‘å®šæ¶æ„
    thread_pool_config = VADThreadPoolConfig(
        max_workers=8,                    # 8ä¸ªå·¥ä½œçº¿ç¨‹
        thread_name_prefix="VADWorker",   # çº¿ç¨‹åç§°å‰ç¼€
        shutdown_timeout_seconds=30.0,   # å…³é—­è¶…æ—¶30ç§’
        warmup_enabled=True,              # å¯ç”¨é¢„çƒ­
        warmup_iterations=3,              # é¢„çƒ­3æ¬¡
        stats_enabled=True                # å¯ç”¨ç»Ÿè®¡
    )
    
    # å¤„ç†å™¨é…ç½® - é«˜æ€§èƒ½è®¾ç½®
    processor_config = VADProcessorConfig(
        audio_config=audio_config,
        vad_config=vad_config,
        thread_pool_config=thread_pool_config,
        buffer_capacity_seconds=3.0,      # 3ç§’ç¼“å†²åŒº
        max_queue_size=64,                # å¤§é˜Ÿåˆ—æ”¯æŒé«˜å¹¶å‘
        enable_performance_monitoring=True
    )
    
    print("âœ… å¹¶è¡Œé…ç½®å®Œæˆ")
    print(f"   - å·¥ä½œçº¿ç¨‹æ•°: {vad_config.workers}")
    print(f"   - é˜Ÿåˆ—å¤§å°: {processor_config.max_queue_size}")
    print(f"   - ç¼“å†²åŒºå®¹é‡: {processor_config.buffer_capacity_seconds}ç§’")
    
    # === 2. åŠ è½½çœŸå®éŸ³é¢‘æ–‡ä»¶ ===
    print("\nğŸµ åŠ è½½çœŸå®éŸ³é¢‘æ–‡ä»¶...")
    
    audio_file = "è¯·é—®ç”µåŠ¨æ±½è½¦å’Œä¼ ç»Ÿæ±½è½¦æ¯”èµ·æ¥å“ªä¸ªæ›´å¥½å•Šï¼Ÿ.wav"
    audio_data = await load_real_audio_file(audio_file)
    
    # === 3. åˆå§‹åŒ–VADå¤„ç†å™¨ ===
    print("\nğŸ¤– åˆå§‹åŒ–é«˜å¹¶å‘VADå¤„ç†å™¨...")
    
    start_time = time.time()
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = await create_vad_processor(
        audio_config=audio_config,
        vad_config=vad_config,
        processor_config=processor_config
    )
    
    init_time = time.time() - start_time
    print(f"âœ… å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ: {init_time:.3f}ç§’")
    
    try:
        # === 4. é«˜å¹¶å‘æµå¼å¤„ç† ===
        print("\nâš¡ å¼€å§‹é«˜å¹¶å‘å¹¶è¡ŒVADæµå¼å¤„ç†...")
        
        # åˆ›å»ºéŸ³é¢‘æµç”Ÿæˆå™¨
        chunk_size = 4096  # è¾ƒå°çš„å—å¢åŠ å¹¶è¡Œåº¦
        stream_generator = AudioStreamGenerator(audio_data, chunk_size, 16000)
        
        # æ€§èƒ½ç›‘æ§å™¨
        monitor = ParallelPerformanceMonitor()
        
        print(f"ğŸ“¦ éŸ³é¢‘æµé…ç½®: {len(audio_data)//chunk_size + 1}ä¸ªå— x {chunk_size/16000*1000:.0f}ms")
        print("ğŸ” å¹¶è¡ŒVADæ£€æµ‹ç»“æœ:")
        print("=" * 60)
        
        # å¼€å§‹å¹¶è¡Œæµå¼å¤„ç†
        processing_start = time.time()
        speech_segments = []
        
        async with processor:
            audio_stream = stream_generator.generate_stream()
            
            async for result in processor.process_stream(audio_stream):
                monitor.add_result(result)
                
                # å®æ—¶æ˜¾ç¤ºç»“æœ
                time_str = f"{result.start_ms/1000:.2f}-{result.end_ms/1000:.2f}s"
                if result.is_speech:
                    status = "ğŸ—£ï¸ è¯­éŸ³"
                    speech_segments.append({
                        'start': result.start_ms/1000,
                        'end': result.end_ms/1000,
                        'probability': result.probability
                    })
                else:
                    status = "ğŸ”‡ é™éŸ³"
                
                print(f"{status} | {time_str} | æ¦‚ç‡: {result.probability:.3f} | å—ID: {result.chunk_id}")
        
        processing_time = time.time() - processing_start
        
        # === 5. å¹¶è¡Œå¤„ç†ç»“æœåˆ†æ ===
        print("=" * 60)
        print("\nğŸ“Š é«˜å¹¶å‘å¹¶è¡Œå¤„ç†ç»“æœåˆ†æ:")
        
        # åŸºæœ¬ç»Ÿè®¡
        stats = monitor.get_parallel_statistics()
        print(f"  ğŸ¯ åŸºæœ¬ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"    - {key}: {value}")
        
        # è¯­éŸ³æ®µç»Ÿè®¡
        if speech_segments:
            print(f"\n  ğŸ¤ æ£€æµ‹åˆ°çš„è¯­éŸ³æ®µ:")
            for i, segment in enumerate(speech_segments, 1):
                duration = segment['end'] - segment['start']
                print(f"    {i}. {segment['start']:.2f}s - {segment['end']:.2f}s "
                      f"(æ—¶é•¿: {duration:.2f}s, æ¦‚ç‡: {segment['probability']:.3f})")
            
            total_speech_duration = sum(s['end'] - s['start'] for s in speech_segments)
            speech_ratio = total_speech_duration / (len(audio_data)/16000) * 100
            print(f"    æ€»è¯­éŸ³æ—¶é•¿: {total_speech_duration:.2f}s ({speech_ratio:.1f}%)")
        
        # === 6. å¤„ç†å™¨æ€§èƒ½æŒ‡æ ‡ ===
        metrics = processor.get_performance_metrics()
        
        print(f"\n  âš¡ å¤„ç†å™¨å†…éƒ¨æ€§èƒ½æŒ‡æ ‡:")
        print(f"    - å¤„ç†å—æ•°: {metrics.success_count + metrics.error_count}")
        print(f"    - å¹³å‡å»¶è¿Ÿ: {metrics.avg_latency_ms:.2f}ms")
        print(f"    - ååé‡: {metrics.throughput_qps:.1f} QPS")
        print(f"    - æ´»è·ƒçº¿ç¨‹æ•°: {metrics.active_threads}")
        print(f"    - é˜Ÿåˆ—æ·±åº¦: {metrics.queue_depth}")
        print(f"    - ç¼“å†²åŒºåˆ©ç”¨ç‡: {metrics.buffer_utilization:.1%}")
        print(f"    - é›¶æ‹·è´ç‡: {metrics.zero_copy_rate:.1%}")
        print(f"    - é”™è¯¯ç‡: {metrics.error_rate:.3f}")
        
    finally:
        # æ¸…ç†èµ„æº
        await processor.close()
    
    print("\nğŸ‰ é«˜å¹¶å‘å¹¶è¡ŒVADå¤„ç†å®Œæˆï¼")
    print("\nğŸ’¡ å¹¶è¡Œå¤„ç†ç‰¹æ€§:")
    print("  - ä½¿ç”¨å®Œæ•´çš„VADProcessorè¿›è¡Œæµå¼å¤„ç†")
    print("  - 8ä¸ªå·¥ä½œçº¿ç¨‹å¹¶è¡ŒVADæ¨ç†ï¼ˆ1:1:1ç»‘å®šæ¶æ„ï¼‰")
    print("  - å¼‚æ­¥éŸ³é¢‘æµå¤„ç†å’ŒèƒŒå‹æ§åˆ¶")
    print("  - é›¶æ‹·è´å†…å­˜ç®¡ç†å’Œé«˜æ€§èƒ½ç¼“å†²åŒº")
    print("  - å®æ—¶æ€§èƒ½ç›‘æ§å’Œæµæ§æœºåˆ¶")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ Cascadeé«˜å¹¶å‘å¹¶è¡ŒVADå¤„ç†å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # è¿è¡Œå¹¶è¡Œæ¼”ç¤º
    await parallel_vad_demo()


if __name__ == "__main__":
    # è¿è¡Œå¹¶è¡Œæ¼”ç¤º
    asyncio.run(main())